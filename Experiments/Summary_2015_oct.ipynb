{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Notebook\n",
    "\n",
    "What we found and where to look for it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Tracking\n",
    "\n",
    "1. Fix loglikelihood:\n",
    "    2. Check if it works or not: gather both LL and EM fits on a given subset of Bayes09 for random parameters and check their correlation.\n",
    "        3. Adapt FitExperiment, as it currently expects one parameter setting and tests it on multiple datasets. It's best to run different sweeps for different datasets, they are not compatible. The handling of n_items N is not great. Should also handle evaluating metrics over a subset of the data better, check if we can do a sampling like that as well (just hack it) \n",
    "            * **Done, now need to rewrite the launchers**\n",
    "            * **Launchers working and finished**\n",
    "    2. Verify if the sigma_output works or not. \n",
    "        * **Couldn't see an issue in the code. Need to launch optim and see the effect**\n",
    "        * **In summary: not really, it affects the EM fit of precision too much.**\n",
    "        * **See following notebook: [Check Loglikelihood model selection behaviour](./fitexperiment_allt_new_10_2015/notebook_LLcheck_random_fitexperimentallt_sigmaxMratiosigmaoutput_121015.ipynb)**\n",
    "    2. If sigma_output doesn't work, use lapse rate instead.\n",
    "        3. Compute new likelihood function, involves a sum of the previous one and a baseline, I think.\n",
    "            * **Done and implemented, seems to be correct**\n",
    "            * **Ran experimental fits, and [obtained a better result](./fitexperiment_allt_new_10_2015/notebook_LLlapse_bays09_random_fitexperimentallt_sigmaxMratiolapserate_131015.ipynb) than with sigma_output. It seems like this could work to get good fits. **\n",
    "        3. Do same thing for Gorgo11.\n",
    "            * ** Got [acceptable fits](./fitexperiment_allt_new_10_2015/notebook_LLlapse_gorgo11_random_fitexperimentallt_sigmaxMratiolapserate_301015.ipynb) as well, with similar characteristics. It is a harder dataset to fit though, as we've seen multiple times... **\n",
    "    2. Single experiment CMA/ES fits, compare with previous fits.\n",
    "        3. First try to run LL90-only optimisations, hope for the best.\n",
    "            * **Seems to find weird parameters values to get highest fits (lapse_rate=.2, sigma_x=0, M=600, ratio=0.4). These give very bad summary statistics, with super high kappa... **\n",
    "            * **Trying new runs with summedLL now. Actually got good results, similar to prodLL... Should have analysed these first? See [notebook](./fitexperiment_allt_cmaes_11_2015/notebook_fitexperimentallt_cmaes_3try_221115.ipynb). Maybe the main point really was the parameter bounds, but then the story about the mismatch of LL ranges between nitems still feels weird to me, I liked that explanation... **\n",
    "            * ** Ran another time, got same results. Parameters are pretty stable, lapse low with LL90. **\n",
    "        3. Secondly, could try to do a combination of LL90 and EM fits? Not the best scales though...\n",
    "            * ** Nah, dropped idea **\n",
    "        4. Trying to run with LL instead of LL90, to check if it's really just the parameter bounds that matter...\n",
    "            * ** See results in this [notebook](./fitexperiment_allt_cmaes_11_2015/notebook_fitexperimentallt_cmaes_4try_280316.ipynb). **\n",
    "            * ** We now need a high Lapse rate. Which shows up in the samples then... Hence there really is a need to avoid fitting some points using our model, in order to get appropriate parameters for sampling and subsequent EM Fits. **\n",
    "            * ** [August 2016] See if per-Subject fits provide an explanation for the required lapse/LL90 flexibility. **\n",
    "    2. CMA/ES with summed LL overfits nitem=1 instead of all. Should move towards a geometric mean instead. Check why that is.\n",
    "        * **The Geometric mean one gives nice fits: Notebook: [CMA/ES with geometric mean](./fitexperiment_allt_cmaes_11_2015/notebook_fitexperimentallt_cmaes_prodLL_1st_231215.ipynb) **\n",
    "        * **But Peter is not convinced by it, it's not a clean setup. Doing a more constrained Sweep, or making a weighted average could be cleaner. **\n",
    "        * ** Check carefully what happens with the re-run of 3try, also see what 4try does, where we use LL instead of LL90. Then we'll know if the issue was really the parameter bounds + chance. If so, would need to be careful next time. Continue by running the sweep that we discussed with Peter. Then just stick with what parameters we find. **\n",
    "        * ** The overall good parameters for Bays09 are actually quite consistent: (lapse_rate=0.0, sigma_x=0.1, M=50, ratio=0.9) or (M=40, ratio=1.0, lapse=0.15 for prodLL)**\n",
    "    2. Do a CMA/ES fit with both lapse_rate and sigmaoutput, and good constraints for the other parameters. Then see what happens with summed LL.\n",
    "        * ** CHECKME: Running, to be analysed there: [Notebook](./fitexperiment_allt_cmaes_11_2015/notebook_fitexperimentallt_cmaes_5try_280316.ipynb) **\n",
    "1. Run fits per subject\n",
    "    2. FitExperimentAllTSubject is ready now, need to check it [16.08.16]\n",
    "    2. Start CMA/ES sweeps:\n",
    "        3. One sweep per subject. Get all optim trajectories.\n",
    "        3. Check parameters per subject. Compare to Mixture Fits per subject.\n",
    "        3. Compute mean parameter across subjects. Compare to previously found best parameter of ** notebook_fitexperimentallt_sigmabase\\_cmaes\\_\\* **\n",
    "        3. Fit Mixture model on samples of mean_parameter. Compare it to the mean of the fits found just above. See if that provides an explanation for the required LL90/lapse rate. This could be a way to argue for \"our model is good but the subjects have different strategies\".\n",
    "\n",
    "1. Sequential Data fits\n",
    "    2. Need to run CMA/ES runs for sequential Gorgo11. We kinda have this for the collapsed_power_law stuff, but I think it's random sweeps.\n",
    "        * **CHECKME: look at required work for sequential Gorgo11 fits **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIMECARD\n",
    "* [06.10.15] 1.A.a underway\n",
    "* [09.10.15] 1.A.a done.\n",
    "* [13.10.15] 1.B done.\n",
    "* [14.10.15] 1.C done.\n",
    "* [29.10.15] 1.D underway.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes 09\n",
    "\n",
    "Fits done with summary statistics work properly. See paper for final ones.\n",
    "These were done through either:\n",
    "\n",
    "*  **[sigmaoutput_normalisedsigmax_random/reloader_normalisedsigmaxsigmaoutput_random_fitmixturemodels_sigmaxMratiosigmaoutput_280814.py](./sigmaoutput_normalisedsigmax_random/reloader_normalisedsigmaxsigmaoutput_random_fitmixturemodels_sigmaxMratiosigmaoutput_280814.py)**\n",
    "* **[fit_mixturemodels/generator_fit_mixturemodels_newresponsemaxout_sigmaxMratio_random_130714.py](fit_mixturemodels/generator_fit_mixturemodels_newresponsemaxout_sigmaxMratio_random_130714.py)**\n",
    "\n",
    "Check which one to be sure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Loglikelihood fits\n",
    "\n",
    "* Using Sigma_output, doesn't really work : [Check Loglikelihood model selection behaviour - 12.1015](./fitexperiment_allt_new_10_2015/notebook_LLcheck_random_fitexperimentallt_sigmaxMratiosigmaoutput_121015.ipynb)\n",
    "* Using Lapse rate, works better : [Check Loglikelihood with Lapse Rate - 13.10.15](./fitexperiment_allt_new_10_2015/notebook_LLlapse_random_fitexperimentallt_sigmaxMratiolapserate_131015.ipynb)\n",
    "\n",
    "The Lapse rate one discusses the issue with using the mean LL to find the fits ==> It overrepresents nitem=1, because the range of LL values are not matched (and good LL for nitem=1 isn't the same as good LL for nitem>1).\n",
    "* See this CMA/ES notebook for the parameters obtained: [notebook_fitexperimentallt_cmaes_2ndtry_091115](./fitexperiment_allt_cmaes_11_2015/notebook_fitexperimentallt_cmaes_2ndtry_091115.ipynb)\n",
    "\n",
    "I tried to change from the average LL to a geometric mean with normalization instead. This is used in the CMA/ES fits instead:\n",
    "* Geometric mean CMA/ES fits: [notebook_fitexperimentallt_cmaes_prodLL_1st_231215](./fitexperiment_allt_cmaes_11_2015/notebook_fitexperimentallt_cmaes_prodLL_1st_231215.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gorgo 11\n",
    "\n",
    "Fits with summary statistics are not great.\n",
    "Basically it's really hard to get parameters that capture both the width of the posterior and the mixture proportions...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loglikelihood fits\n",
    "\n",
    "* Done with Lapse rate, works all right: [Loglikelihood Lapse Rate for Gorgo 11](./fitexperiment_allt_new_10_2015/notebook_LLlapse_gorgo11_random_fitexperimentallt_sigmaxMratiolapserate_301015.ipynb). But not amazing, similar as with summary statistics fits. Dataset is harder to fit in general...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
